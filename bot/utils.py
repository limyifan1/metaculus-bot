"""Utility functions for the multi-agent forecasting system."""

from __future__ import annotations

import re
from dataclasses import dataclass
import logging
from typing import Dict, Iterable, List
from datetime import datetime, timezone

import numpy as np
from scipy.interpolate import PchipInterpolator

logger = logging.getLogger(__name__)


def today_iso_utc() -> str:
    """Return today's date in ISO format using UTC (YYYY-MM-DD)."""
    return datetime.now(timezone.utc).strftime("%Y-%m-%d")


def extract_probability_from_response_as_percentage_not_decimal(text: str) -> float:
    """Extract a probability expressed as a percentage and return a decimal."""
    match = re.search(r"([0-9]+(?:\.[0-9]+)?)%", text)
    if not match:
        logger.warning(
            "[Committee][Utils] No percentage found in binary response. Defaulting to 0.5. Text=%s",
            text,
        )
        return 0.5
    return float(match.group(1)) / 100.0


def extract_option_probabilities_from_response(text: str) -> List[float]:
    """Parse a list of probabilities from a string like ``[30%, 40%, 30%]``."""
    match = re.search(r"\[([^\]]+)\]", text)
    if not match:
        logger.warning(
            "[Committee][Utils] Could not find list in multiple choice response. Text=%s",
            text,
        )
        return []
    parts = match.group(1).split(",")
    probs = []
    for part in parts:
        number_match = re.search(r"([0-9]+(?:\.[0-9]+)?)", part)
        if number_match:
            probs.append(float(number_match.group(1)) / 100.0)
    return probs


def normalize_probabilities(probs: Iterable[float]) -> List[float]:
    """Normalize a probability list while enforcing API bounds.

    Metaculus requires each probability to lie within ``[0.001, 0.999]``
    (and, for multi-option questions, the values must sum to ``1``). The
    responses generated by the LLMs can occasionally contain negative values,
    non-finite numbers, or extremely skewed distributions (e.g. ``[100%, 0%,
    0%]``). If we submit those directly we hit a ``400`` response from the
    forecasts API. This helper projects the raw numbers onto a valid
    probability simplex by:

    * dropping NaNs / negative numbers,
    * normalizing the remaining mass,
    * mixing with a uniform prior just enough to satisfy the bounds.

    The mixture guarantees a sum of one while staying inside the required
    bounds, preventing downstream runtime errors when publishing forecasts.
    """

    arr = np.array(list(probs), dtype=float)
    n = arr.size
    if n == 0:
        return []

    # Replace NaNs / infinities and clip negatives.
    arr = np.where(np.isfinite(arr), arr, 0.0)
    arr = np.clip(arr, 0.0, None)

    total = arr.sum()
    if total <= 0:
        logger.warning(
            "[Committee][Utils] Non-positive probability sum detected. Uniform fallback applied. Values=%s",
            arr.tolist(),
        )
        return [1.0 / n] * n

    arr /= total

    # Determine feasible bounds. For very large option counts the API minimum
    # may be unattainable, so fall back to the uniform probability.
    min_prob = min(0.001, 1.0 / n)
    max_prob = 1.0 - min_prob * (n - 1)
    if max_prob < min_prob:
        return [1.0 / n] * n

    uniform = np.full(n, 1.0 / n)

    # Compute the amount of uniform mixing required so that every probability
    # satisfies the [min_prob, max_prob] constraints. The value ``alpha`` in
    # [0, 1] controls the convex combination: ``(1-alpha)*arr + alpha*uniform``.
    alpha = 0.0
    for p, u in zip(arr, uniform):
        if p < min_prob - 1e-12:
            denom = u - p
            if denom <= 0:
                alpha = 1.0
                break
            alpha = max(alpha, (min_prob - p) / denom)
        elif p > max_prob + 1e-12:
            denom = p - u
            if denom <= 0:
                alpha = 1.0
                break
            alpha = max(alpha, (p - max_prob) / denom)

    alpha = float(np.clip(alpha, 0.0, 1.0))
    adjusted = (1.0 - alpha) * arr + alpha * uniform

    # Numerical tolerances can still produce tiny violations. Iterate a couple
    # of times to clip and renormalize, which converges rapidly given the
    # narrow bounds.
    for _ in range(3):
        adjusted = np.clip(adjusted, min_prob, max_prob)
        total = adjusted.sum()
        if total <= 0:
            return [1.0 / n] * n
        adjusted /= total
        if (
            np.all(adjusted >= min_prob - 1e-12)
            and np.all(adjusted <= max_prob + 1e-12)
            and abs(adjusted.sum() - 1.0) <= 1e-12
        ):
            break

    return adjusted.tolist()


def extract_percentiles_from_response(text: str) -> Dict[int, float]:
    """Extract ``{percentile: value}`` pairs from lines like ``Percentile 10: 45``."""
    result: Dict[int, float] = {}
    for line in text.splitlines():
        match = re.search(r"(\d+)[a-z]{0,2}\s*[:=]\s*([0-9]+(?:\.[0-9]+)?)", line, re.I)
        if match:
            result[int(match.group(1))] = float(match.group(2))
    return result


@dataclass
class PercentileForecast:
    percentiles: Dict[int, float]

    def generate_continuous_cdf(self) -> List[float]:
        """Generate a 201 point monotonic CDF using PCHIP interpolation."""
        if not self.percentiles:
            raise ValueError("No percentiles provided")
        items = sorted(self.percentiles.items())
        xs = np.array([p / 100.0 for p, _ in items])
        ys = np.array([v for _, v in items])
        interpolator = PchipInterpolator(xs, ys, extrapolate=True)
        grid = np.linspace(0.0, 1.0, 201)
        values = interpolator(grid)
        monotonic = np.maximum.accumulate(values).tolist()
        logger.debug(
            "[Committee][Utils] Generated continuous CDF with %s points (min=%.3f, max=%.3f)",
            len(monotonic),
            float(monotonic[0]) if monotonic else float("nan"),
            float(monotonic[-1]) if monotonic else float("nan"),
        )
        return monotonic
